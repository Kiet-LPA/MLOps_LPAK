from flask import Flask, render_template, request
import mlflow.pyfunc
import mlflow
import numpy as np
import os

# Thiáº¿t láº­p MLflow tracking URI (cho Docker container)
mlflow_tracking_uri = os.environ.get('MLFLOW_TRACKING_URI', './mlruns')
mlflow.set_tracking_uri(mlflow_tracking_uri)
print(f"ğŸ“Œ MLflow tracking URI: {mlflow_tracking_uri}")

app = Flask(__name__)

# Load model - sá»­ dá»¥ng nhiá»u phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ Ä‘áº£m báº£o hoáº¡t Ä‘á»™ng trong Docker
# Model version 1 Ä‘Æ°á»£c Ä‘Äƒng kÃ½ tá»« run_id: 545cfe034e9f4902944e82b745e5e7a7
# experiment_id: 602464189259425114
model = None
# Sá»­ dá»¥ng base path tá»« MLFLOW_TRACKING_URI hoáº·c máº·c Ä‘á»‹nh
base_mlruns_path = mlflow_tracking_uri if os.path.isabs(mlflow_tracking_uri) else os.path.join(os.getcwd(), mlflow_tracking_uri)

# ÄÆ°á»ng dáº«n tuyá»‡t Ä‘á»‘i Ä‘áº¿n model artifacts trong container
direct_model_path = os.path.join(base_mlruns_path, "602464189259425114/models/m-5c70dc42de7f4e63ad68f8ad473ae8f4/artifacts")

model_paths = [
    # Æ¯u tiÃªn 1: Load trá»±c tiáº¿p tá»« artifacts folder (hoáº¡t Ä‘á»™ng sau khi Ä‘Ã£ fix paths)
    direct_model_path,
    # Æ¯u tiÃªn 2: Load tá»« registry (sau khi Ä‘Ã£ sá»­a Ä‘Æ°á»ng dáº«n Windows trong Dockerfile)
    "models:/lpak_classifier/1",
    # Æ¯u tiÃªn 3: Load tá»« run_id trá»±c tiáº¿p (fallback)
    "runs:/545cfe034e9f4902944e82b745e5e7a7/model",
]

for model_path in model_paths:
    try:
        print(f"ğŸ”„ Attempting to load model from: {model_path}")
        # Kiá»ƒm tra path tá»“n táº¡i náº¿u lÃ  Ä‘Æ°á»ng dáº«n file system
        if isinstance(model_path, str) and not model_path.startswith(("models:", "runs:")):
            if not os.path.exists(model_path):
                print(f"âš ï¸ Path does not exist: {model_path}")
                continue
        
        model = mlflow.pyfunc.load_model(model_path)
        print(f"âœ… Model loaded successfully from: {model_path}")
        break
    except Exception as e:
        print(f"âš ï¸ Failed to load from {model_path}: {str(e)}")
        continue

if model is None:
    print("âŒ Failed to load model from all paths!")
    print("ğŸ’¡ Please ensure mlruns directory is properly copied into Docker container.")
else:
    print("âœ… Model is ready for predictions!")

@app.route("/", methods=["GET", "POST"])
def index():
    prediction = None
    if request.method == "POST":
        try:
            # Äá»c 10 feature Ä‘áº§u vÃ o
            features = [float(request.form[f"f{i}"]) for i in range(1, 11)]
            arr = np.array(features).reshape(1, -1)
            prediction = int(model.predict(arr)[0])
        except Exception as e:
            prediction = f"Lá»—i khi dá»± Ä‘oÃ¡n: {e}"
    return render_template("index.html", prediction=prediction)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
